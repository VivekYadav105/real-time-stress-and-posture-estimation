{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipeNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached mediapipe-0.10.7-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting absl-py (from mediapipe)\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting attrs>=19.1.0 (from mediapipe)\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting flatbuffers>=2.0 (from mediapipe)\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting matplotlib (from mediapipe)\n",
      "  Using cached matplotlib-3.8.1-cp310-cp310-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting numpy (from mediapipe)\n",
      "  Using cached numpy-1.26.1-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Collecting opencv-contrib-python (from mediapipe)\n",
      "  Using cached opencv_contrib_python-4.8.1.78-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting protobuf<4,>=3.11 (from mediapipe)\n",
      "  Using cached protobuf-3.20.3-cp310-cp310-win_amd64.whl (904 kB)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Using cached sounddevice-0.4.6-py3-none-win_amd64.whl (199 kB)\n",
      "Collecting CFFI>=1.0 (from sounddevice>=0.4.4->mediapipe)\n",
      "  Using cached cffi-1.16.0-cp310-cp310-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->mediapipe)\n",
      "  Using cached contourpy-1.2.0-cp310-cp310-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->mediapipe)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->mediapipe)\n",
      "  Using cached fonttools-4.44.0-cp310-cp310-win_amd64.whl.metadata (156 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->mediapipe)\n",
      "  Using cached kiwisolver-1.4.5-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\class\\7th sem\\mini_project\\yolo_v3\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (23.2)\n",
      "Collecting pillow>=8 (from matplotlib->mediapipe)\n",
      "  Using cached Pillow-10.1.0-cp310-cp310-win_amd64.whl.metadata (9.6 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->mediapipe)\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\class\\7th sem\\mini_project\\yolo_v3\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Collecting pycparser (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe)\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\class\\7th sem\\mini_project\\yolo_v3\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Using cached mediapipe-0.10.7-cp310-cp310-win_amd64.whl (50.3 MB)\n",
      "Using cached opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl (38.1 MB)\n",
      "Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Using cached numpy-1.26.1-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "Using cached matplotlib-3.8.1-cp310-cp310-win_amd64.whl (7.6 MB)\n",
      "Using cached opencv_contrib_python-4.8.1.78-cp37-abi3-win_amd64.whl (44.8 MB)\n",
      "Using cached cffi-1.16.0-cp310-cp310-win_amd64.whl (181 kB)\n",
      "Using cached contourpy-1.2.0-cp310-cp310-win_amd64.whl (186 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.44.0-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "Using cached kiwisolver-1.4.5-cp310-cp310-win_amd64.whl (56 kB)\n",
      "Using cached Pillow-10.1.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Installing collected packages: flatbuffers, pyparsing, pycparser, protobuf, pillow, numpy, kiwisolver, fonttools, cycler, attrs, absl-py, opencv-python, opencv-contrib-python, contourpy, CFFI, sounddevice, matplotlib, mediapipe\n",
      "Successfully installed CFFI-1.16.0 absl-py-2.0.0 attrs-23.1.0 contourpy-1.2.0 cycler-0.12.1 flatbuffers-23.5.26 fonttools-4.44.0 kiwisolver-1.4.5 matplotlib-3.8.1 mediapipe-0.10.7 numpy-1.26.1 opencv-contrib-python-4.8.1.78 opencv-python-4.8.1.78 pillow-10.1.0 protobuf-3.20.3 pycparser-2.21 pyparsing-3.1.1 sounddevice-0.4.6\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Using cached ultralytics-8.0.206-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in d:\\class\\7th sem\\mini_project\\yolo_v3\\.venv\\lib\\site-packages (from ultralytics) (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.22.2 in d:\\class\\7th sem\\mini_project\\yolo_v3\\.venv\\lib\\site-packages (from ultralytics) (1.26.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in d:\\class\\7th sem\\mini_project\\yolo_v3\\.venv\\lib\\site-packages (from ultralytics) (4.8.1.78)\n",
      "Requirement already satisfied: pillow>=7.1.2 in d:\\class\\7th sem\\mini_project\\yolo_v3\\.venv\\lib\\site-packages (from ultralytics) (10.1.0)\n",
      "Collecting pyyaml>=5.3.1 (from ultralytics)\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting requests>=2.23.0 (from ultralytics)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting scipy>=1.4.1 (from ultralytics)\n",
      "  Using cached scipy-1.11.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting torch>=1.8.0 (from ultralytics)\n",
      "  Using cached torch-2.1.0-cp310-cp310-win_amd64.whl.metadata (24 kB)\n",
      "Collecting torchvision>=0.9.0 (from ultralytics)\n",
      "  Using cached torchvision-0.16.0-cp310-cp310-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting tqdm>=4.64.0 (from ultralytics)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pandas>=1.1.4 (from ultralytics)\n",
      "  Using cached pandas-2.1.2-cp310-cp310-win_amd64.whl.metadata (18 kB)\n",
      "Collecting seaborn>=0.11.0 (from ultralytics)\n",
      "  Using cached seaborn-0.13.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: psutil in d:\\class\\7th sem\\mini_project\\yolo_v3\\.venv\\lib\\site-packages (from ultralytics) (5.9.6)\n",
      "Collecting py-cpuinfo (from ultralytics)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting thop>=0.1.1 (from ultralytics)\n",
      "  Using cached thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\class\\7th sem\\mini_project\\yolo_v3\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\class\\7th sem\\mini_project\\yolo_v3\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\class\\7th sem\\mini_project\\yolo_v3\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\class\\7th sem\\mini_project\\yolo_v3\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\class\\7th sem\\mini_project\\yolo_v3\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\class\\7th sem\\mini_project\\yolo_v3\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\class\\7th sem\\mini_project\\yolo_v3\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.1.4->ultralytics)\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas>=1.1.4->ultralytics)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.23.0->ultralytics)\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.23.0->ultralytics)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.23.0->ultralytics)\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.23.0->ultralytics)\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting filelock (from torch>=1.8.0->ultralytics)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typing-extensions (from torch>=1.8.0->ultralytics)\n",
      "  Using cached typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch>=1.8.0->ultralytics)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch>=1.8.0->ultralytics)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting fsspec (from torch>=1.8.0->ultralytics)\n",
      "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: colorama in d:\\class\\7th sem\\mini_project\\yolo_v3\\.venv\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in d:\\class\\7th sem\\mini_project\\yolo_v3\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.8.0->ultralytics)\n",
      "  Using cached MarkupSafe-2.1.3-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.8.0->ultralytics)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached ultralytics-8.0.206-py3-none-any.whl (644 kB)\n",
      "Using cached pandas-2.1.2-cp310-cp310-win_amd64.whl (10.7 MB)\n",
      "Using cached PyYAML-6.0.1-cp310-cp310-win_amd64.whl (145 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached scipy-1.11.3-cp310-cp310-win_amd64.whl (44.1 MB)\n",
      "Using cached seaborn-0.13.0-py3-none-any.whl (294 kB)\n",
      "Using cached torch-2.1.0-cp310-cp310-win_amd64.whl (192.3 MB)\n",
      "Using cached torchvision-0.16.0-cp310-cp310-win_amd64.whl (1.3 MB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n",
      "Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Using cached MarkupSafe-2.1.3-cp310-cp310-win_amd64.whl (17 kB)\n",
      "Installing collected packages: pytz, py-cpuinfo, mpmath, urllib3, tzdata, typing-extensions, tqdm, sympy, scipy, pyyaml, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, requests, pandas, jinja2, torch, seaborn, torchvision, thop, ultralytics\n",
      "Successfully installed MarkupSafe-2.1.3 certifi-2023.7.22 charset-normalizer-3.3.2 filelock-3.13.1 fsspec-2023.10.0 idna-3.4 jinja2-3.1.2 mpmath-1.3.0 networkx-3.2.1 pandas-2.1.2 py-cpuinfo-9.0.0 pytz-2023.3.post1 pyyaml-6.0.1 requests-2.31.0 scipy-1.11.3 seaborn-0.13.0 sympy-1.12 thop-0.1.1.post2209072238 torch-2.1.0 torchvision-0.16.0 tqdm-4.66.1 typing-extensions-4.8.0 tzdata-2023.3 ultralytics-8.0.206 urllib3-2.0.7\n"
     ]
    }
   ],
   "source": [
    "%pip install mediapipe opencv-python\n",
    "%pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import sys\n",
    "from ultralytics import YOLO\n",
    "import math as m\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "#path to the yolo models\n",
    "face_model = YOLO(r\"face_detector.pt\")\n",
    "person_model = YOLO(r\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal camera feed\n",
    "cap = cv2.VideoCapture(1)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('Mediapipe Feed', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Determining Joints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/3j8BPdc.png\" style=\"height:300px\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calculate Angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle \n",
    "\n",
    "def calculate_shoulder_angle(a,b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    \n",
    "    radians = np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAngle(x1, y1, x2, y2):\n",
    "    theta = m.acos((y2 - y1) * (-y1) / (m.sqrt(\n",
    "        (x2 - x1) ** 2 + (y2 - y1) ** 2) * y1))\n",
    "    degree = int(180 / m.pi) * theta\n",
    "    return degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. custom pose plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " plot_pose(image,landmarks):\n",
    "    fordef landmark in landmarks:\n",
    "        cv2.circle(image,(int(landmark[0]),int(landmark[1])),3,(0, 0, 255),-1)\n",
    "    \n",
    "\n",
    "    cv2.line(image, (int(landmarks[8][0]),int(landmarks[8][1])), (int(landmarks[6][0]),int(landmarks[6][1])), (0, 255, 0), 2)   \n",
    "    cv2.line(image, (int(landmarks[6][0]),int(landmarks[6][1])), (int(landmarks[5][0]),int(landmarks[5][1])), (0, 255, 0), 2)    \n",
    "    cv2.line(image, (int(landmarks[5][0]),int(landmarks[5][1])), (int(landmarks[4][0]),int(landmarks[4][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[4][0]),int(landmarks[4][1])), (int(landmarks[0][0]),int(landmarks[0][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[0][0]),int(landmarks[0][1])), (int(landmarks[1][0]),int(landmarks[1][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[1][0]),int(landmarks[1][1])), (int(landmarks[2][0]),int(landmarks[2][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[2][0]),int(landmarks[2][1])), (int(landmarks[3][0]),int(landmarks[3][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[3][0]),int(landmarks[3][1])), (int(landmarks[7][0]),int(landmarks[7][1])), (0, 255, 0), 2)  \n",
    "\n",
    "\n",
    "    #mouth line\n",
    "    cv2.line(image, (int(landmarks[10][0]),int(landmarks[10][1])), (int(landmarks[9][0]),int(landmarks[9][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[9][0]),int(landmarks[9][1])), (int(landmarks[10][0]),int(landmarks[10][1])), (0, 255, 0), 2)  \n",
    "  \n",
    "    #main torse\n",
    "    cv2.line(image, (int(landmarks[12][0]),int(landmarks[12][1])), (int(landmarks[11][0]),int(landmarks[11][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[12][0]),int(landmarks[12][1])), (int(landmarks[24][0]),int(landmarks[24][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[24][0]),int(landmarks[24][1])), (int(landmarks[23][0]),int(landmarks[23][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[11][0]),int(landmarks[11][1])), (int(landmarks[23][0]),int(landmarks[23][1])), (0, 255, 0), 2)  \n",
    "\n",
    "    #legs\n",
    "    cv2.line(image, (int(landmarks[24][0]),int(landmarks[24][1])), (int(landmarks[26][0]),int(landmarks[26][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[26][0]),int(landmarks[26][1])), (int(landmarks[28][0]),int(landmarks[28][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[23][0]),int(landmarks[23][1])), (int(landmarks[25][0]),int(landmarks[25][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[25][0]),int(landmarks[25][1])), (int(landmarks[27][0]),int(landmarks[27][1])), (0, 255, 0), 2)  \n",
    "    \n",
    "    #foot1\n",
    "    cv2.line(image, (int(landmarks[28][0]),int(landmarks[28][1])), (int(landmarks[32][0]),int(landmarks[32][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[32][0]),int(landmarks[32][1])), (int(landmarks[30][0]),int(landmarks[30][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[28][0]),int(landmarks[28][1])), (int(landmarks[30][0]),int(landmarks[30][1])), (0, 255, 0), 2)  \n",
    "\n",
    "    #foot2\n",
    "    cv2.line(image, (int(landmarks[27][0]),int(landmarks[27][1])), (int(landmarks[29][0]),int(landmarks[29][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[29][0]),int(landmarks[29][1])), (int(landmarks[31][0]),int(landmarks[31][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[31][0]),int(landmarks[31][1])), (int(landmarks[27][0]),int(landmarks[27][1])), (0, 255, 0), 2)  \n",
    "\n",
    "    #hand1\n",
    "    cv2.line(image, (int(landmarks[12][0]),int(landmarks[12][1])), (int(landmarks[14][0]),int(landmarks[14][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[14][0]),int(landmarks[14][1])), (int(landmarks[16][0]),int(landmarks[16][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[16][0]),int(landmarks[16][1])), (int(landmarks[18][0]),int(landmarks[18][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[20][0]),int(landmarks[20][1])), (int(landmarks[18][0]),int(landmarks[18][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[20][0]),int(landmarks[20][1])), (int(landmarks[16][0]),int(landmarks[16][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[22][0]),int(landmarks[22][1])), (int(landmarks[16][0]),int(landmarks[16][1])), (0, 255, 0), 2)  \n",
    "\n",
    "    #hand1\n",
    "    cv2.line(image, (int(landmarks[11][0]),int(landmarks[11][1])), (int(landmarks[13][0]),int(landmarks[13][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[13][0]),int(landmarks[13][1])), (int(landmarks[15][0]),int(landmarks[15][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[15][0]),int(landmarks[15][1])), (int(landmarks[21][0]),int(landmarks[21][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[15][0]),int(landmarks[15][1])), (int(landmarks[19][0]),int(landmarks[19][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[19][0]),int(landmarks[19][1])), (int(landmarks[17][0]),int(landmarks[17][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[15][0]),int(landmarks[15][1])), (int(landmarks[17][0]),int(landmarks[17][1])), (0, 255, 0), 2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Custom Pose Estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_estimate(landmarks):\n",
    "    left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "    right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "    left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "    right_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "\n",
    "    angle = calculate_shoulder_angle(left_shoulder,right_shoulder)\n",
    "    angle1 = calculate_angle(left_shoulder,right_shoulder,right_elbow)\n",
    "    angle2 = calculate_angle(right_shoulder,left_shoulder,left_elbow)\n",
    "            \n",
    "    \n",
    "            \n",
    "    shoulder_posture = angle >= 0 and angle <= 10\n",
    "    left_arm_posture = angle1 >= 90 and angle1 <= 124\n",
    "    right_arm_posture = angle2 >= 90 and angle2 <= 124\n",
    "    if shoulder_posture and left_arm_posture and right_arm_posture:\n",
    "        stage = \"Good\"\n",
    "        suggestion = None\n",
    "    if shoulder_posture and not(left_arm_posture and right_arm_posture):\n",
    "        stage=\"Bad\"\n",
    "        suggestion = \"adjust your arm posture\"\n",
    "    if not(shoulder_posture) and (left_arm_posture and right_arm_posture):\n",
    "        stage = \"Bad\"\n",
    "        suggestion = \"adjust your shoulder posture\"\n",
    "    if not(shoulder_posture) and not(left_arm_posture and right_arm_posture):\n",
    "        stage = \"Bad\"\n",
    "        suggestion = \"adjust your posture\"\n",
    "        \n",
    "    return [stage,angle,angle1,angle2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multi-person pose detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Pose detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "with mp_pose.Pose(static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    smooth_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read() \n",
    "        image = frame   \n",
    "        predicted = person_model(image,stream=True)\n",
    "        for r in predicted:\n",
    "            boxes=r.boxes\n",
    "            for box in boxes:\n",
    "                x1,y1,x2,y2=box.xyxwh[0]\n",
    "                x1,y1,x2,y2=int(x1), int(y1), int(x2), int(y2)\n",
    "                cropped_image = image[y1:y2,x1:x2]\n",
    "                cropped_image = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "                cropped_image.flags.writeable = False\n",
    "                cv2.rectangle(frame, (x1,y1), (x2,y2), (255,0,255),3)\n",
    "                cropped_image = np.ascontiguousarray(cropped_image)\n",
    "                results = pose.process(cropped_image)\n",
    "                try:\n",
    "                    landmarks = results.pose_landmarks.landmark\n",
    "                    adjusted_landmarks = [(lm.x1 * x2 + x1, lm.y1 *y2  + y1) for lm in results.pose_landmarks.landmark]\n",
    "                    plot_pose(image,adjusted_landmarks)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    pass\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"results\\mutiple person pose.png\" style=\"width:300px;height:440px;margin:auto\">\n",
    "<img src=\"results\\mutiple pose estimation.jpg\" style=\"width:650px;height:440px;margin:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Front and side combined version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "## Setup mediapipe instance\n",
    "stage = None\n",
    "suggestion = None\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        h, w = frame.shape[:2]\n",
    "\n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "      \n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "    \n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Get coordinates\n",
    "            left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "            left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            right_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "            left_ear = [landmarks[mp_pose.PoseLandmark.LEFT_EAR.value].x,landmarks[mp_pose.PoseLandmark.LEFT_EAR.value].y]\n",
    "            left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "            \n",
    "            # Calculate angle\n",
    "            angle = calculate_shoulder_angle(left_shoulder,right_shoulder)\n",
    "            angle1 = calculate_angle(left_shoulder,right_shoulder,right_elbow)\n",
    "            angle2 = calculate_angle(right_shoulder,left_shoulder,left_elbow)\n",
    "            \n",
    "            \n",
    "            # Visualize angle  \n",
    "            cv2.putText(image, str(round(angle,3)), \n",
    "                           tuple(np.multiply(left_shoulder, [450, 450]).astype(int)), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                                )\n",
    "            cv2.putText(image, str(round(angle1,3)), \n",
    "                           tuple(np.multiply(right_shoulder, [600, 480]).astype(int)), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                                )\n",
    "            cv2.putText(image, str(round(angle2,3)), \n",
    "                           tuple(np.multiply(left_shoulder, [600, 480]).astype(int)), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                                )\n",
    "            \n",
    "            #drawing joints\n",
    "            cv2.line(image, (int(left_shoulder[0]*w), int(left_shoulder[1]*h)), (int(right_shoulder[0]*w),int(right_shoulder[1]*h)), (127, 255, 0), 2)\n",
    "            cv2.line(image, (int(left_shoulder[0]*w), int(left_shoulder[1]*h)), (int(left_elbow[0]*w),int(left_elbow[1]*h)), (127, 255, 0), 2)\n",
    "            cv2.line(image, (int(right_shoulder[0]*w),int(right_shoulder[1]*h)),(int(right_elbow[0]*w), int(right_elbow[1]*h)), (127, 255, 0), 2)\n",
    "            \n",
    "            shoulder_posture = angle >= 0 and angle <= 10\n",
    "            left_arm_posture = angle1 >= 90 and angle1 <= 124\n",
    "            right_arm_posture = angle2 >= 90 and angle2 <= 124\n",
    "            if shoulder_posture and left_arm_posture and right_arm_posture:\n",
    "                stage = \"Good\"\n",
    "                suggestion = None\n",
    "            if shoulder_posture and not(left_arm_posture and right_arm_posture):\n",
    "                if(left_arm_posture):\n",
    "                    cv2.line(image, (int(left_elbow[0]*w), int(left_elbow[1]*h)), (int(left_shoulder[0]*w),int(left_shoulder[1]*h)), (50, 50, 255), 2)\n",
    "                if(right_arm_posture):\n",
    "                    cv2.line(image, (int(right_elbow[0]*w), int(right_elbow[1]*h)), (int(right_shoulder[0]*w),int(right_shoulder[1]*h)), (50, 50, 255), 2)\n",
    "                if(not(left_arm_posture or right_arm_posture)):\n",
    "                    cv2.line(image, (int(left_elbow[0]*w), int(left_elbow[1]*h)), (int(left_shoulder[0]*w),int(left_shoulder[1]*h)), (50, 50, 255), 2)\n",
    "                    cv2.line(image, (int(right_elbow[0]*w), int(right_elbow[1]*h)), (int(right_shoulder[0]*w),int(right_shoulder[1]*h)), (50, 50, 255), 2)\n",
    "                stage=\"Bad\"\n",
    "                suggestion = \"adjust your arm posture\"\n",
    "            if not(shoulder_posture) and (left_arm_posture and right_arm_posture):\n",
    "                cv2.line(image, (int(left_shoulder[0]*w), int(left_shoulder[1]*h)), (int(right_shoulder[0]*w),int(right_shoulder[1]*h)), (50, 50, 255), 2)\n",
    "                stage = \"Bad\"\n",
    "                suggestion = \"adjust your shoulder posture\"\n",
    "            if not(shoulder_posture) and not(left_arm_posture and right_arm_posture):\n",
    "                stage = \"Bad\"\n",
    "                suggestion = \"adjust your posture\"\n",
    " \n",
    "\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "         # Setup status box\n",
    "        cv2.rectangle(image, (0,0), (225,73), (245,117,16), -1)\n",
    "        \n",
    "        cv2.putText(image, stage, \n",
    "                    (20,35), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,255,255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, suggestion, \n",
    "                    (20,60), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "            \n",
    "        # Render detections\n",
    "#         mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "#                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "#                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "#                                  )               \n",
    "        \n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"results\\IMG-20231106-WA0011.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Stress detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 utility function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = image / 255.0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Stress detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"stress_detector.h5\"\n",
    "    model = tf.keras.models.load_model(model_path, custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "    cap = cv2.VideoCapture(1)\n",
    "\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        results = face_model(frame, stream = True)\n",
    "        for r in results:\n",
    "            boxes=r.boxes\n",
    "            for box in boxes:\n",
    "                x1,y1,x2,y2=box.xyxy[0]\n",
    "                x1,y1,x2,y2=int(x1), int(y1), int(x2), int(y2)\n",
    "                cv2.rectangle(frame,[x1,y1],[x2,y2],(255,0,0),2)\n",
    "                cropped_frame = frame[y1:y2,x1:x2]\n",
    "                processed_frame = preprocess_image(cropped_frame)\n",
    "                prediction = model.predict(processed_frame)\n",
    "                label = \"not stressed\" if prediction[0][0] > 0.5 else \"stressed\"\n",
    "                cv2.putText(frame, label, (x1,y1), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"results\\IMG-20231106-WA0010.jpg\" style=\"width:500px;height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Stress and posture combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x576 4 faces, 1266.1ms\n",
      "Speed: 68.0ms preprocess, 1266.1ms inference, 241.3ms postprocess per image at shape (1, 3, 640, 576)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x576 2 persons, 1 chair, 329.6ms\n",
      "Speed: 61.5ms preprocess, 329.6ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 576)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math as m\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "\n",
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle \n",
    "\n",
    "def calculate_shoulder_angle(a,b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    \n",
    "    radians = np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle \n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Resize the image to match the input size of the model\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    # Convert the image to a format that can be input to the model\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    # Normalize the pixel values\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "def findAngle(x1, y1, x2, y2):\n",
    "    theta = m.acos((y2 - y1) * (-y1) / (m.sqrt(\n",
    "        (x2 - x1) ** 2 + (y2 - y1) ** 2) * y1))\n",
    "    degree = int(180 / m.pi) * theta\n",
    "    return degree\n",
    "\n",
    "def plot_pose(image,landmarks):\n",
    "    for landmark in landmarks:\n",
    "        cv2.circle(image,(int(landmark[0]),int(landmark[1])),3,(0, 0, 255),-1)\n",
    "    cv2.line(image, (int(landmarks[8][0]),int(landmarks[8][1])), (int(landmarks[6][0]),int(landmarks[6][1])), (0, 255, 0), 2)   \n",
    "    cv2.line(image, (int(landmarks[6][0]),int(landmarks[6][1])), (int(landmarks[5][0]),int(landmarks[5][1])), (0, 255, 0), 2)    \n",
    "    cv2.line(image, (int(landmarks[5][0]),int(landmarks[5][1])), (int(landmarks[4][0]),int(landmarks[4][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[4][0]),int(landmarks[4][1])), (int(landmarks[0][0]),int(landmarks[0][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[0][0]),int(landmarks[0][1])), (int(landmarks[1][0]),int(landmarks[1][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[1][0]),int(landmarks[1][1])), (int(landmarks[2][0]),int(landmarks[2][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[2][0]),int(landmarks[2][1])), (int(landmarks[3][0]),int(landmarks[3][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[3][0]),int(landmarks[3][1])), (int(landmarks[7][0]),int(landmarks[7][1])), (0, 255, 0), 2)  \n",
    "\n",
    "\n",
    "    #mouth line\n",
    "    cv2.line(image, (int(landmarks[10][0]),int(landmarks[10][1])), (int(landmarks[9][0]),int(landmarks[9][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[9][0]),int(landmarks[9][1])), (int(landmarks[10][0]),int(landmarks[10][1])), (0, 255, 0), 2)  \n",
    "  \n",
    "    #main torse\n",
    "    cv2.line(image, (int(landmarks[12][0]),int(landmarks[12][1])), (int(landmarks[11][0]),int(landmarks[11][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[12][0]),int(landmarks[12][1])), (int(landmarks[24][0]),int(landmarks[24][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[24][0]),int(landmarks[24][1])), (int(landmarks[23][0]),int(landmarks[23][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[11][0]),int(landmarks[11][1])), (int(landmarks[23][0]),int(landmarks[23][1])), (0, 255, 0), 2)  \n",
    "\n",
    "    #legs\n",
    "    cv2.line(image, (int(landmarks[24][0]),int(landmarks[24][1])), (int(landmarks[26][0]),int(landmarks[26][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[26][0]),int(landmarks[26][1])), (int(landmarks[28][0]),int(landmarks[28][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[23][0]),int(landmarks[23][1])), (int(landmarks[25][0]),int(landmarks[25][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[25][0]),int(landmarks[25][1])), (int(landmarks[27][0]),int(landmarks[27][1])), (0, 255, 0), 2)  \n",
    "    \n",
    "    #foot1\n",
    "    cv2.line(image, (int(landmarks[28][0]),int(landmarks[28][1])), (int(landmarks[32][0]),int(landmarks[32][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[32][0]),int(landmarks[32][1])), (int(landmarks[30][0]),int(landmarks[30][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[28][0]),int(landmarks[28][1])), (int(landmarks[30][0]),int(landmarks[30][1])), (0, 255, 0), 2)  \n",
    "\n",
    "    #foot2\n",
    "    cv2.line(image, (int(landmarks[27][0]),int(landmarks[27][1])), (int(landmarks[29][0]),int(landmarks[29][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[29][0]),int(landmarks[29][1])), (int(landmarks[31][0]),int(landmarks[31][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[31][0]),int(landmarks[31][1])), (int(landmarks[27][0]),int(landmarks[27][1])), (0, 255, 0), 2)  \n",
    "\n",
    "    #hand1\n",
    "    cv2.line(image, (int(landmarks[12][0]),int(landmarks[12][1])), (int(landmarks[14][0]),int(landmarks[14][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[14][0]),int(landmarks[14][1])), (int(landmarks[16][0]),int(landmarks[16][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[16][0]),int(landmarks[16][1])), (int(landmarks[18][0]),int(landmarks[18][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[20][0]),int(landmarks[20][1])), (int(landmarks[18][0]),int(landmarks[18][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[20][0]),int(landmarks[20][1])), (int(landmarks[16][0]),int(landmarks[16][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[22][0]),int(landmarks[22][1])), (int(landmarks[16][0]),int(landmarks[16][1])), (0, 255, 0), 2)  \n",
    "\n",
    "    #hand1\n",
    "    cv2.line(image, (int(landmarks[11][0]),int(landmarks[11][1])), (int(landmarks[13][0]),int(landmarks[13][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[13][0]),int(landmarks[13][1])), (int(landmarks[15][0]),int(landmarks[15][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[15][0]),int(landmarks[15][1])), (int(landmarks[21][0]),int(landmarks[21][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[15][0]),int(landmarks[15][1])), (int(landmarks[19][0]),int(landmarks[19][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[19][0]),int(landmarks[19][1])), (int(landmarks[17][0]),int(landmarks[17][1])), (0, 255, 0), 2)  \n",
    "    cv2.line(image, (int(landmarks[15][0]),int(landmarks[15][1])), (int(landmarks[17][0]),int(landmarks[17][1])), (0, 255, 0), 2)  \n",
    "\n",
    "\n",
    "def pose_estimate(landmarks):\n",
    "    left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "    right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "    left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "    right_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "\n",
    "    angle = calculate_shoulder_angle(left_shoulder,right_shoulder)\n",
    "    angle1 = calculate_angle(left_shoulder,right_shoulder,right_elbow)\n",
    "    angle2 = calculate_angle(right_shoulder,left_shoulder,left_elbow)\n",
    "            \n",
    "    \n",
    "            \n",
    "    shoulder_posture = angle >= 0 and angle <= 10\n",
    "    left_arm_posture = angle1 >= 90 and angle1 <= 124\n",
    "    right_arm_posture = angle2 >= 90 and angle2 <= 124\n",
    "    if shoulder_posture and left_arm_posture and right_arm_posture:\n",
    "        stage = \"Good\"\n",
    "        suggestion = None\n",
    "    if shoulder_posture and not(left_arm_posture and right_arm_posture):\n",
    "        stage=\"Bad\"\n",
    "        suggestion = \"adjust your arm posture\"\n",
    "    if not(shoulder_posture) and (left_arm_posture and right_arm_posture):\n",
    "        stage = \"Bad\"\n",
    "        suggestion = \"adjust your shoulder posture\"\n",
    "    if not(shoulder_posture) and not(left_arm_posture and right_arm_posture):\n",
    "        stage = \"Bad\"\n",
    "        suggestion = \"adjust your posture\"\n",
    "        \n",
    "    return [stage,angle,angle1,angle2]\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use OpenCV to capture video from the webcam\n",
    "    model_path = \"stress_detector.h5\"\n",
    "    model = tf.keras.models.load_model(model_path, custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "    yolo_face = YOLO('face_detector.pt')\n",
    "    person_model = YOLO(r\"yolov8n.pt\")\n",
    "    frame = cv2.imread(r'test-images\\test-image-1.jpg')\n",
    "    cv2.resize(frame,(640,640),frame,cv2.INTER_LINEAR)\n",
    "    with mp_pose.Pose(static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    smooth_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as pose:\n",
    "        new_frame = frame\n",
    "        face_results = yolo_face(frame)\n",
    "        for r in face_results:\n",
    "            boxes=r.boxes\n",
    "            for box in boxes:\n",
    "                x1,y1,x2,y2=box.xyxy[0]\n",
    "                x1,y1,x2,y2=int(x1), int(y1), int(x2), int(y2)\n",
    "                cv2.rectangle(frame,[x1,y1],[x2,y2],(255,0,0),2)\n",
    "                cropped_frame = frame[y1:y2,x1:x2]\n",
    "                processed_frame = preprocess_image(cropped_frame)\n",
    "                prediction = model.predict(processed_frame)\n",
    "                label = \"not stressed\" if prediction[0][0] > 0.5 else \"stressed\"\n",
    "                cv2.putText(new_frame, label, (x1,y1), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        \n",
    "        person_results = person_model(frame)\n",
    "        for r in person_results:\n",
    "            boxes = r.boxes\n",
    "            for box in boxes:\n",
    "                if box.cls[0]==0:\n",
    "                    x,y,x1,y1=box.xyxy[0]\n",
    "                    x,y,x1,y1=int(x), int(y), int(x1), int(y1)\n",
    "                    cropped_image = frame[y:y1,x:x1]\n",
    "                    cv2.rectangle(new_frame, (x,y), (x1,y1), (255,0,255),3)\n",
    "                    cropped_image = np.ascontiguousarray(cropped_image)\n",
    "                    cropped_image.flags.writeable = True\n",
    "                    cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_RGB2BGR)\n",
    "                    results = pose.process(cropped_image)\n",
    "                    print(results)\n",
    "                    try:\n",
    "                        landmarks = results.pose_landmarks.landmark\n",
    "                        adjusted_landmarks = [(lm.x * (x1-x) + x, lm.y * (y1-y) + y) for lm in results.pose_landmarks.landmark]\n",
    "                        plot_pose(new_frame,adjusted_landmarks)\n",
    "                        [stage,angle,angle1,angle2] = pose_estimate(landmarks)\n",
    "                        color = (0,255,0)\n",
    "                        if stage == \"Bad\":\n",
    "                            color = (0,0,255)\n",
    "                        cv2.putText(new_frame,str(round(angle1,3)),(int(adjusted_landmarks[12][0]),int(adjusted_landmarks[12][1])),cv2.FONT_HERSHEY_SIMPLEX, .5, color, 1, cv2.LINE_AA)\n",
    "                        cv2.putText(new_frame,str(round(angle2,3)),(int(adjusted_landmarks[11][0]),int(adjusted_landmarks[11][1])),cv2.FONT_HERSHEY_SIMPLEX, .5, color, 1, cv2.LINE_AA)\n",
    "                        cv2.putText(new_frame, stage, (x,y), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        pass\n",
    "\n",
    "    cv2.imshow('frame', new_frame)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"results\\combined_1.jpg\" style=\"width:500px;height:500px\">\n",
    "<img src=\"results\\combined_2.jpg\" style=\"width:500px;height:500px\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
